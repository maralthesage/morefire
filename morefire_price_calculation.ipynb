{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/24 08:01:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum,lit,when,format_number,row_number,col,filter\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV Loader\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"8192\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"8192\") \\\n",
    "    .config(\"spark.executor.cores\", \"7\") \\\n",
    "    .config(\"spark.driver.cores\", \"7\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morefire_file ='/Users/maralsheikhzadeh/Documents/Codes/Analytics/morefire/HG_Salesvalidierung_15-12_31-12-2024.xls'\n",
    "df_first = pd.read_excel(morefire_file,header=0)\n",
    "df_first['ID'] = df_first['ID'].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      173445929630439\n",
       "1      173454275706439\n",
       "2      173443750595039\n",
       "3      173523857094339\n",
       "4      173459742533739\n",
       "            ...       \n",
       "227    173476523319039\n",
       "228    173529574879739\n",
       "229    173442649115239\n",
       "230    173546499724939\n",
       "231    173433937845239\n",
       "Name: ID, Length: 232, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Originals first\n",
    "With Server-connect folder **dbf_to_spark.ipynb**, first update all the csv files below so that we can find the EXT_Verweis in them. The following fields connect the data from all 4 countries into 3 unified tables to further connect them with the file from Morefire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096 = spark.read.csv('/Volumes/MARAL/CSV/F01/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056 = spark.read.csv('/Volumes/MARAL/CSV/F01/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156 = spark.read.csv('/Volumes/MARAL/CSV/F01/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096_FR = spark.read.csv('/Volumes/MARAL/CSV/f02/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056_FR = spark.read.csv('/Volumes/MARAL/CSV/f02/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156_FR = spark.read.csv('/Volumes/MARAL/CSV/f02/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096_AU = spark.read.csv('/Volumes/MARAL/CSV/F03/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056_AU = spark.read.csv('/Volumes/MARAL/CSV/F03/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156_AU = spark.read.csv('/Volumes/MARAL/CSV/F03/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096_CH = spark.read.csv('/Volumes/MARAL/CSV/F04/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056_CH = spark.read.csv('/Volumes/MARAL/CSV/F04/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156_CH = spark.read.csv('/Volumes/MARAL/CSV/F04/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096 = V2AD1096_FR.union(V2AD1096_AU).union(V2AD1096_CH).union(V2AD1096)\n",
    "V2AD1056 = V2AD1056_FR.union(V2AD1056_AU).union(V2AD1056_CH).union(V2AD1056)\n",
    "V2AD1156 = V2AD1156_FR.union(V2AD1156_AU).union(V2AD1156_CH).union(V2AD1156)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096 = V2AD1096.withColumn('SHOPNUMMER', col('SHOPNUMMER').cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/24 08:01:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 15:====================================>                    (7 + 4) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+---------+------+---------------+-------+---------+----------+---------+----------+----------+----------+----------+--------+-------+---------+--------+--------+---------+--------+----------+-------+---------+-------+----------+--------+----------+\n",
      "|      RECH_NR|   BEST_AM|BEST_VOM|LIEFERUNG|STATUS|     SHOPNUMMER|SYS_ART|EXT_AUFNR|EXT_RECHNR|KOMMENTAR|RESELLERCP|FREIE_KZ01|FREIE_KZ02|FREIE_KZ03|REF_TEXT|PARTNER|BEST_ZEIT|ZAHLTEXT|SB_NAMEN|SB_ANZAHL|INDISTAT|VORDTM_AUF|DFUE_KZ|LI_NUMMER|LI_DATE|    NUMMER|SHOPZAHL|REFERRER_C|\n",
      "+-------------+----------+--------+---------+------+---------------+-------+---------+----------+---------+----------+----------+----------+----------+--------+-------+---------+--------+--------+---------+--------+----------+-------+---------+-------+----------+--------+----------+\n",
      "|5417278400000|2024-10-11|    NULL|     NULL|  NULL|172867735995939|   NULL|     NULL|      NULL|     NULL|       0.0|      NULL|      NULL|      NULL|    NULL|      0| 22:09:20|    NULL|    NULL|        0|    NULL|      NULL|      0|     NULL|   NULL|0006282907|      00|M000007957|\n",
      "|5417278403CD2|      NULL|    NULL|     NULL|  NULL|172867735995939|   NULL|     NULL|      NULL|     NULL|       0.0|      NULL|      NULL|      NULL|    NULL|   NULL|     NULL|    NULL|    NULL|        0|    NULL|      NULL|      0|     NULL|   NULL|0006282907|      00|      NULL|\n",
      "+-------------+----------+--------+---------+------+---------------+-------+---------+----------+---------+----------+----------+----------+----------+--------+-------+---------+--------+--------+---------+--------+----------+-------+---------+-------+----------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# filtered_df = V2AD1096.filter(col(\"SHOPNUMMER\").contains(\"172867735995939\"))\n",
    "# filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df_first)\n",
    "df = df.join(V2AD1096[['SHOPNUMMER','RECH_NR']],df['ID'] == V2AD1096['SHOPNUMMER'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df['RECH_NR'] = merged_df['RECH_NR'].astype(str)\n",
    "df = df.withColumn('RECH_NR', col('RECH_NR').cast(StringType()))\n",
    "V2AD1056 = V2AD1056.withColumn('RECH_NR', col('RECH_NR').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(V2AD1056[['VERWEIS','RECH_NR','BEST_WERT','MWST1','AUF_ANLAGE']],on='RECH_NR',how='left')\n",
    "df = df.join(V2AD1156[['ART_NR','PREIS','MWST','RECHNUNG']],df['RECH_NR']==V2AD1156['RECHNUNG'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing from Pyspark to Pandas\n",
    "I used Pyspark to load and connect the data from our databases because the data is so large and Pandas is so slow. Now that everything is joined and the data is filtered to the original lines we had, it's easier to transform it back to Pandas to do the final analysis on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=41570Kb max_used=41685Kb free=89501Kb\n",
      " bounds [0x000000010c9e8000, 0x000000010f2d8000, 0x00000001149e8000]\n",
      " total_blobs=13943 nmethods=13010 adapters=840\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()\n",
    "# df_new[df_new['SHOPNUMMER'].str.contains('172867735995939',na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data\n",
    "\n",
    "- We create a columnn for items that were **Not Found** in the VS4 Data. We make a * sign for them to know what is missing from data.\n",
    "- The third line removes items that were not computed in Morefire's Sales_Betrag, namely: Versandkosten (VK) and Geschenkverpackung (011P00). Items may need to be added to this list in the future depending on the items that we see further that are not computed here.\n",
    "- Then we filter only the columns we need. and change data types for our requirements.\n",
    "- Finally, we aggregate all the lines from the same Rechnung and Extra so that we compute the Fakturierte Nettoumsatz based on the Nettoumsatz value for each product in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Publisher_ID'] = df_new['Publisher_ID'].astype(str)\n",
    "df_new['Not Found'] = \"\"\n",
    "df_new.loc[df_new['SHOPNUMMER'].isna(),'Not Found'] = '*'\n",
    "\n",
    "df_new = df_new[df_new['ART_NR'].isin(['VK','011P00'])==False]\n",
    "# df_new = df_new[['ID', 'AWIN ID', 'Publisher_ID', 'Sale_Betrag', 'Provision', 'Datum',\n",
    "#        'Site_Name', 'URL', 'Ablehnungs_Grund', 'Referrer', 'Publisher_URL',\n",
    "#        'Transaktions_Teile', 'Land_nach_IP', 'Benutzerdef_Parameter',\n",
    "#        'Produkte', 'getrackte_Teile', 'Netzwerkgebühr','Not Found']]\n",
    "df_new['PREIS'] = df_new['PREIS'].astype(float).fillna(0)\n",
    "df_new = df_new.drop_duplicates(subset=['ID','Datum','PREIS','ART_NR'])\n",
    "df_grouped = df_new.groupby(\n",
    "    ['ID']\n",
    ").agg({'PREIS': 'sum','Not Found':'first'}).reset_index().rename(columns={'PREIS': 'Fakturierte Nettoumsatz'})\n",
    "df_grouped = df_grouped.merge(df_first,on='ID',how='left')\n",
    "df_grouped = df_grouped[['ID', 'AWIN ID', 'Publisher_ID', 'Sale_Betrag', 'Provision', 'Datum',\n",
    "       'Site_Name', 'URL', 'Ablehnungs_Grund', 'Referrer', 'Publisher_URL',\n",
    "       'Transaktions_Teile', 'Land_nach_IP', 'Benutzerdef_Parameter',\n",
    "       'Produkte', 'getrackte_Teile', 'Netzwerkgebühr','Fakturierte Nettoumsatz','Not Found']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the difference between our nettoumsatz and what is given by Morefire as Sales_Betrag and leave it in the Difference column.\n",
    "When everything is computed. we save the data in an excel sheet and there do some minor formatting like formatting the data as a table and then using formatting cell to add Euro sign to columns with Euro values and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_grouped['Fakturierte Nettoumsatz'] = df_grouped['Fakturierte Nettoumsatz'].fillna(0)\n",
    "\n",
    "# 3. Create 'Difference' column\n",
    "def calculate_difference(row):\n",
    "    sale_betrag = row['Sale_Betrag']\n",
    "    fakturierte_nettoumsatz = row['Fakturierte Nettoumsatz']\n",
    "    difference = sale_betrag - fakturierte_nettoumsatz\n",
    "    return '' if round(difference, 0) == 0 else f\"{round(difference, 2):.2f}\"\n",
    "\n",
    "df_grouped['Difference'] = df_grouped.apply(calculate_difference, axis=1)\n",
    "df_grouped.to_excel('morefire_list.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
