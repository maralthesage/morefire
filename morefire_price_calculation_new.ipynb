{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum,lit,when,format_number,row_number,col,filter\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV Loader\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"8192\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"8192\") \\\n",
    "    .config(\"spark.executor.cores\", \"7\") \\\n",
    "    .config(\"spark.driver.cores\", \"7\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morefire_file ='/Users/maralsheikhzadeh/documents/codes/Repeating-Analytics/morefire/HG_Salesvalidierung_01-31Mai25.xlsx'\n",
    "data = []\n",
    "for sheet_name in ['DE','AT','FR','CH']:\n",
    "    df = pd.read_excel(morefire_file,header=0,sheet_name=sheet_name)\n",
    "    df['ID'] = df['ID'].fillna(0).astype(int).astype(str)\n",
    "    df['LAND'] = sheet_name\n",
    "    data.append(df)\n",
    "\n",
    "df_first = pd.concat(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Originals first\n",
    "With Server-connect folder **dbf_to_spark.ipynb**, first update all the csv files below so that we can find the EXT_Verweis in them. The following fields connect the data from all 4 countries into 3 unified tables to further connect them with the file from Morefire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096 = spark.read.csv('/Volumes/MARAL/CSV/F01/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056 = spark.read.csv('/Volumes/MARAL/CSV/F01/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156 = spark.read.csv('/Volumes/MARAL/CSV/F01/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096_FR = spark.read.csv('/Volumes/MARAL/CSV/f02/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056_FR = spark.read.csv('/Volumes/MARAL/CSV/f02/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156_FR = spark.read.csv('/Volumes/MARAL/CSV/f02/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096_AU = spark.read.csv('/Volumes/MARAL/CSV/F03/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056_AU = spark.read.csv('/Volumes/MARAL/CSV/F03/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156_AU = spark.read.csv('/Volumes/MARAL/CSV/F03/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096_CH = spark.read.csv('/Volumes/MARAL/CSV/F04/V2AD1096.csv',sep=';',header=True)\n",
    "V2AD1056_CH = spark.read.csv('/Volumes/MARAL/CSV/F04/V2AD1056.csv',sep=';',header=True)\n",
    "V2AD1156_CH = spark.read.csv('/Volumes/MARAL/CSV/F04/V2AD1156.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096 = V2AD1096_FR.union(V2AD1096_AU).union(V2AD1096_CH).union(V2AD1096)\n",
    "V2AD1056 = V2AD1056_FR.union(V2AD1056_AU).union(V2AD1056_CH).union(V2AD1056)\n",
    "V2AD1156 = V2AD1156_FR.union(V2AD1156_AU).union(V2AD1156_CH).union(V2AD1156)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2AD1096 = V2AD1096.withColumn('SHOPNUMMER', col('SHOPNUMMER').cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df_first)\n",
    "df = df.join(V2AD1096[['SHOPNUMMER','RECH_NR']],df['ID'] == V2AD1096['SHOPNUMMER'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df['RECH_NR'] = merged_df['RECH_NR'].astype(str)\n",
    "df = df.withColumn('RECH_NR', col('RECH_NR').cast(StringType()))\n",
    "V2AD1056 = V2AD1056.withColumn('RECH_NR', col('RECH_NR').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(V2AD1056[['VERWEIS','RECH_NR','BEST_WERT','MWST1','AUF_ANLAGE']],on='RECH_NR',how='left')\n",
    "df = df.join(V2AD1156[['ART_NR','GROESSE','FARBE','PREIS','MWST','RECHNUNG']],df['RECH_NR']==V2AD1156['RECHNUNG'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing from Pyspark to Pandas\n",
    "I used Pyspark to load and connect the data from our databases because the data is so large and Pandas is so slow. Now that everything is joined and the data is filtered to the original lines we had, it's easier to transform it back to Pandas to do the final analysis on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GROESSE'] = df['GROESSE'].astype(str).fillna(\"\")\n",
    "df['FARBE'] = df['FARBE'].astype(str).fillna(\"\")\n",
    "df['ANR'] = df['ART_NR'].str.ljust(8) + df['GROESSE'].str.ljust(4) + df['FARBE'].str.ljust(2)\n",
    "df['ANR'] = df['ANR'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data\n",
    "\n",
    "- We create a columnn for items that were **Not Found** in the VS4 Data. We make a * sign for them to know what is missing from data.\n",
    "- The third line removes items that were not computed in Morefire's Sales_Betrag, namely: Versandkosten (VK) and Geschenkverpackung (011P00). Items may need to be added to this list in the future depending on the items that we see further that are not computed here.\n",
    "- Then we filter only the columns we need. and change data types for our requirements.\n",
    "- Finally, we aggregate all the lines from the same Rechnung and Extra so that we compute the Fakturierte Nettoumsatz based on the Nettoumsatz value for each product in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Publisher_ID'] = df_new['Publisher_ID'].astype(str)\n",
    "df_new['Not Found'] = \"\"\n",
    "df_new.loc[df_new['SHOPNUMMER'].isna(),'Not Found'] = '*'\n",
    "\n",
    "df_new = df_new[df_new['ART_NR'].isin(['VK','011P00'])==False]\n",
    "# df_new = df_new[['ID', 'AWIN ID', 'Publisher_ID', 'Sale_Betrag', 'Provision', 'Datum',\n",
    "#        'Site_Name', 'URL', 'Ablehnungs_Grund', 'Referrer', 'Publisher_URL',\n",
    "#        'Transaktions_Teile', 'Land_nach_IP', 'Benutzerdef_Parameter',\n",
    "#        'Produkte', 'getrackte_Teile', 'Netzwerkgebühr','Not Found']]\n",
    "df_new['PREIS'] = df_new['PREIS'].astype(float).fillna(0)\n",
    "df_new = df_new.drop_duplicates(subset=['ID','Datum','PREIS','ANR','RECHNUNG'])\n",
    "df_grouped = df_new.groupby(\n",
    "    ['ID']\n",
    ").agg({'PREIS': 'sum','Not Found':'first'}).reset_index().rename(columns={'PREIS': 'Fakturierte Nettoumsatz'})\n",
    "df_grouped = df_grouped.merge(df_first,on='ID',how='left')\n",
    "df_grouped = df_grouped[['ID', 'AWIN ID', 'Publisher_ID', 'Sale_Betrag', 'Provision', 'Datum',\n",
    "       'Site_Name', 'URL', 'Ablehnungs_Grund', 'Referrer', 'Publisher_URL',\n",
    "       'Transaktions_Teile', 'Land_nach_IP', 'Benutzerdef_Parameter',\n",
    "       'Produkte', 'getrackte_Teile', 'Netzwerkgebühr','Fakturierte Nettoumsatz','Not Found',\"LAND\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the difference between our nettoumsatz and what is given by Morefire as Sales_Betrag and leave it in the Difference column.\n",
    "When everything is computed. we save the data in an excel sheet and there do some minor formatting like formatting the data as a table and then using formatting cell to add Euro sign to columns with Euro values and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_grouped['Fakturierte Nettoumsatz'] = df_grouped['Fakturierte Nettoumsatz'].fillna(0)\n",
    "# df_grouped.loc[df_grouped['Fakturierte Nettoumsatz']<0, 'Fakturierte Nettoumsatz'] = 0 \n",
    "    \n",
    "# 3. Create 'Difference' column\n",
    "def calculate_difference(row):\n",
    "    sale_betrag = row['Sale_Betrag']\n",
    "    fakturierte_nettoumsatz = row['Fakturierte Nettoumsatz']\n",
    "    difference = sale_betrag - fakturierte_nettoumsatz\n",
    "    return '' if round(difference, 0) == 0 else f\"{round(difference, 2):.2f}\"\n",
    "\n",
    "df_grouped['Difference'] = df_grouped.apply(calculate_difference, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_grouped.to_excel('morefire_list.xlsx', index=False)\n",
    "\n",
    "with pd.ExcelWriter('MoreFire_Export.xlsx', engine='openpyxl') as writer:\n",
    "    for land in ['DE','AT','FR','CH']:\n",
    "        df_final = df_grouped[df_grouped['LAND'] == land].copy()\n",
    "        df_final.drop(columns=['LAND'],inplace=True)\n",
    "        df_final.to_excel(writer,sheet_name=land,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
